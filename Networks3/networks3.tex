\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{float} % Allows use of [H] float option
\usepackage{amsfonts} % For \mathbb
\usepackage{amsmath}

\title{Networks 3}
\author{Haim Lavi, 038712105}
\date{December 2025}

\begin{document}

\maketitle

\section{}
Code is in https://github.com/HaimL76/Networks.git
\section{}
All the plots in this section are for the BA model without fitness.
\subsection{}
Graph of the average degree as function of the time / number of nodes. We get almost immediately $\langle{k}\rangle=2m$, except for $t=0$ where $\langle{k}\rangle=m-1$.
\begin{figure}[H]
    \makebox[\linewidth]{
\includegraphics[width=1.85\linewidth]{ba_model_k_average_n.png}
    }
\end{figure}
\subsection{}
Graph of the degree ratio as function of the square root of the time / number of nodes.
\begin{figure}[H]
    \makebox[\linewidth]{
\includegraphics[width=1.75\linewidth]{ba_model_k_ratio_n.png}
}
\end{figure}
\subsection{}
Graph of the degree of several nodes as function of the time / number of nodes (in log-log scale).
\begin{figure}[H]
    \makebox[\linewidth]{
\includegraphics[width=1.75\linewidth]{ba_model_ki_t_loglog.png}
    }
\end{figure}
\subsection{}
Graph of the degree distribution (in log-log scale, with the calculated distribution). Notice that since our network is finite, then $\mathbb{P}[k]$ is either zero or $\geq\frac{1}{N}$ (either no node has this $k$, or at least one does), so taking only degrees that appear in the graph, we get the line of $\mathbb{P}[k]$ to turn away from the calculated slope and fix around $\frac{1}{N}$, for the highest degrees.
\begin{figure}[H]
    \makebox[\linewidth]{
\includegraphics[width=1.75\linewidth]{ba_model_p_k_loglog_with_slope.png}
    }
\end{figure}
Graph of the degree distribution (in log-log scale, with log binning, with the calculated distribution). We need to divide the density of each bin in the total sum of degrees, to get the log binning close enough to the calculated degree distribution.
\begin{figure}[H]
    \makebox[\linewidth]{
\includegraphics[width=1.75\linewidth]{ba_model_p_k_loglog_binning_with_slope.png}
    }
\end{figure}
Graph of the degree distribution (in log-log scale, with log binning taking medians only, with the calculated distribution).
\begin{figure}[H]
    \makebox[\linewidth]{
\includegraphics[width=1.75\linewidth]{ba_model_p_k_loglog_binning_with_slope_take_medians.png}
    }
\end{figure}
\section{}
We use the same analysis method, $\frac{dk_i}{dt}=\mathbb{E}[k_i(t)]=m\pi_i(t)=m\frac{k_i\eta_i}{\sum_{j=1}^tk_j\eta_j}$, where $\eta_i$ is fixed for every node $i$ (the node that appears at $t_i$). Taking $\eta_i=\alpha$ for all $i$, where $\alpha$ is some constant, reduces to the BA preferential attachment model. Taking $\eta_i>\eta_j$ for every $i<j$ only increases the first-mover advantage in this model, because then $\pi_i>>\pi_j$, so $\eta_i$ is not a valid fitness. So we must take $\eta_j>\eta_i$ for every $i<j$. But in order to have the fitter nodes match or surpass the first-mover nodes, we need $\pi_1\leq\pi_2\leq\cdots\leq\pi_N$.

I found the general analysis particularly difficult, so I picked the fitness function $\eta_i=i$.

$k_1(1)=m$.

$k_1(2)=2m$, $k_2(2)=m$.

So if we take $\eta_1=1$ and $\eta_2=2$, then for step $3$ we get 

$\pi_1(3)=\frac{k_1(2)\eta_2}{\sum_{i=1}^2}=\frac{2m\cdot{1}}{4m}=\frac{1}{2}$.

$\pi_2(3)=\frac{k_2(2)\eta_2}{\sum_{i=1}^2}=\frac{m\cdot{2}}{4m}=\frac{1}{2}$.

For step $4$ we get

$\pi_1(4)=\frac{k_1(3)\eta_1}{\sum_{i=1}^3}=\frac{(2m+\frac{m}{2})\cdot{1}}{\sum_{i=1}^3}=\frac{\frac{5m}{2}}{\sum_{i=1}^3}$.

$\pi_2(4)=\frac{k_2(3)\eta_2}{\sum_{i=1}^3}=\frac{(m+\frac{m}{2})\cdot{2}}{\sum_{i=1}^3}=\frac{\frac{3m}{2}\cdot{2}}{\sum_{i=1}^3}=\frac{3m}{\sum_{i=1}^3}$.

$\pi_3(4)=\frac{k_3(3)\eta_3}{\sum_{i=1}^3}=\frac{3m}{\sum_{i=1}^3}$.

Moving further with this calculation will show that a portion of fitter nodes will match their predecessor nodes in probability after they appear, and will surpass them in next steps.
Taking greater fitness functions ($\xi_i\geq\eta_i$, for every node $i$) will obviously result in a greater advantage for the fitter nodes. However, notice that there is always a tail of nodes that will have lower degrees, even for higher fitness values, because at each step, $\frac{dk_i}{dt}(t)=\mathbb{E}[k_i(t)]=m\pi_i(t)\leq{m}$, so the latest late-mover nodes will not be able to match their predecessors.

Graph of $K_i(t)$, for several nodes $i$, where $\eta_i=i$
\begin{figure}[H]
    \makebox[\linewidth]{
\includegraphics[width=1.75\linewidth]{ba_model_ki_t_loglog_fitness_linear.png}
    }
\end{figure}
Graph of $K_i(t)$, for several nodes $i$, where $\eta_i=2^i$
\begin{figure}[H]
    \makebox[\linewidth]{
\includegraphics[width=1.75\linewidth]{ba_model_ki_t_loglog_fitness_square.png}
    }
\end{figure}
\end{document}
